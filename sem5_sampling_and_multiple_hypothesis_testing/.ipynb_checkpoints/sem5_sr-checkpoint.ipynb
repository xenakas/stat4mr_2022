{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f746dba",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2506f79a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-17T10:45:11.813898Z",
     "start_time": "2022-10-17T10:45:11.700666Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"sample\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8da7ff",
   "metadata": {},
   "source": [
    "#### Simple Random Sampling\n",
    "\n",
    "Say you want to select a subset of a population in which each member of the subset has an equal probability of being chosen.\n",
    "\n",
    "Below we select 100 sample points from a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6b129c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-17T10:45:12.043987Z",
     "start_time": "2022-10-17T10:45:12.007167Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_df = df.sample(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ee2080",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-17T10:45:12.461480Z",
     "start_time": "2022-10-17T10:45:12.459397Z"
    }
   },
   "outputs": [],
   "source": [
    "# sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c92d79c",
   "metadata": {},
   "source": [
    "#### Stratified Sampling\n",
    "\n",
    "Assume that we need to estimate the average number of votes for each candidate in an election. Assume that the country has 3 towns:\n",
    "\n",
    "Town A has 1 million factory workers,\n",
    "\n",
    "Town B has 2 million workers, and\n",
    "\n",
    "Town C has 3 million retirees.\n",
    "\n",
    "We can choose to get a random sample of size 60 over the entire population but there is some chance that the random sample turns out to be not well balanced across these towns and hence is biased causing a significant error in estimation.\n",
    "\n",
    "Instead, if we choose to take a random sample of 10, 20 and 30 from Town A, B and C respectively then we can produce a smaller error in estimation for the same total size of the sample.\n",
    "\n",
    "You can do something like this pretty easily with Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbe1237",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-17T10:45:14.186165Z",
     "start_time": "2022-10-17T10:45:14.179149Z"
    }
   },
   "outputs": [],
   "source": [
    "X = df.iloc[:,:2]\n",
    "y = df.iloc[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8c3f43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-17T10:45:15.049363Z",
     "start_time": "2022-10-17T10:45:14.478824Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    stratify=y, \n",
    "                                                    test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733a9735",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-17T10:45:15.462225Z",
     "start_time": "2022-10-17T10:45:15.050945Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c26d63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-17T10:45:17.333836Z",
     "start_time": "2022-10-17T10:45:17.151980Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa27c6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-17T10:45:17.666693Z",
     "start_time": "2022-10-17T10:45:17.547720Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d659808",
   "metadata": {},
   "source": [
    "#### Reservoir Sampling'\n",
    "\n",
    "I love this problem statement:\n",
    "\n",
    "Say you have a stream of items of large and unknown length that we can only iterate over once.\n",
    "\n",
    "Create an algorithm that randomly chooses an item from this stream such that each item is equally likely to be selected.\n",
    "\n",
    "How can we do that?\n",
    "\n",
    "Let us assume we have to sample 5 objects out of an infinite stream such that each element has an equal probability of getting selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0199a74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-17T10:45:19.606717Z",
     "start_time": "2022-10-17T10:45:19.599861Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generator(max):\n",
    "    number = 1\n",
    "    while number < max:\n",
    "        number += 1\n",
    "        yield number\n",
    "# Create as stream generator\n",
    "stream = generator(10000)\n",
    "\n",
    "# Doing Reservoir Sampling from the stream\n",
    "k=5\n",
    "reservoir = []\n",
    "for i, element in enumerate(stream):\n",
    "    if i+1<= k:\n",
    "        reservoir.append(element)\n",
    "    else:\n",
    "        probability = k/(i+1)\n",
    "        if random.random() < probability:\n",
    "            # Select item in stream and remove one of the k items already selected\n",
    "            reservoir[random.choice(range(0,k))] = element\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ba4097",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-17T10:45:20.317980Z",
     "start_time": "2022-10-17T10:45:20.315331Z"
    }
   },
   "outputs": [],
   "source": [
    "print(reservoir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548b34c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-20T09:38:27.630023Z",
     "start_time": "2021-11-20T09:38:27.620632Z"
    }
   },
   "source": [
    "It can be mathematically proved that in the sample each element has the same probability of getting selected from the stream.\n",
    "\n",
    "How?\n",
    "\n",
    "It always helps to think of a smaller problem when it comes to mathematics.\n",
    "\n",
    "So, let us think of a stream of only 3 items and we have to keep 2 of them.\n",
    "\n",
    "We see the first item, we hold it in the list as our reservoir has space. We see the second item, we hold it in the list as our reservoir has space.\n",
    "\n",
    "We see the third item. Here is where things get interesting. We choose the third item to be in the list with probability 2/3.\n",
    "\n",
    "Let us now see the probability of first item getting selected:\n",
    "\n",
    "The probability of removing the first item is the probability of element 3 getting selected multiplied by the probability of Element 1 getting randomly chosen as the replacement candidate from the 2 elements in the reservoir. That probability is:\n",
    "\n",
    "2/3*1/2 = 1/3\n",
    "\n",
    "Thus the probability of 1 getting selected is:\n",
    "\n",
    "1–1/3 = 2/3\n",
    "\n",
    "We can have the exact same argument for the Second Element and we can extend it for many elements.\n",
    "\n",
    "Thus each item has the same probability of getting selected: 2/3 or in general k/n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdbab9b",
   "metadata": {},
   "source": [
    "#### Random Undersampling and Oversampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162d6b3e",
   "metadata": {},
   "source": [
    "It is too often that we encounter an imbalanced dataset.\n",
    "\n",
    "A widely adopted technique for dealing with highly imbalanced datasets is called resampling. It consists of removing samples from the majority class (under-sampling) and/or adding more examples from the minority class (over-sampling).\n",
    "\n",
    "Let us first create some example imbalanced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30216ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-17T10:48:28.835318Z",
     "start_time": "2022-10-17T10:48:28.828449Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_classes=2, class_sep=1.5, weights=[0.9, 0.1],\n",
    "    n_informative=3, n_redundant=1, flip_y=0,\n",
    "    n_features=20, n_clusters_per_class=1,\n",
    "    n_samples=100, random_state=10\n",
    ")\n",
    "X = pd.DataFrame(X)\n",
    "X['target'] = y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23673acd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-20T09:40:11.866459Z",
     "start_time": "2021-11-20T09:40:11.861648Z"
    }
   },
   "source": [
    "We can now do random oversampling and undersampling using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f8f503",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-17T10:48:31.132736Z",
     "start_time": "2022-10-17T10:48:31.124812Z"
    }
   },
   "outputs": [],
   "source": [
    "num_0 = len(X[X['target']==0])\n",
    "num_1 = len(X[X['target']==1])\n",
    "\n",
    "print(num_0,num_1)\n",
    "\n",
    "# random undersample\n",
    "undersampled_data = pd.concat([ X[X['target']==0].sample(num_1) , X[X['target']==1] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f5a02f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-17T10:45:23.960915Z",
     "start_time": "2022-10-17T10:45:23.953352Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(undersampled_data))\n",
    "\n",
    "# random oversample\n",
    "\n",
    "oversampled_data = pd.concat([ X[X['target']==0] , X[X['target']==1].sample(num_0, replace=True) ])\n",
    "print(len(oversampled_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894cb0b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-17T10:50:26.879918Z",
     "start_time": "2022-10-17T10:50:26.874770Z"
    }
   },
   "outputs": [],
   "source": [
    "df1 = df.sample(100)\n",
    "X = df1.iloc[:,:2]\n",
    "y = df1.iloc[:,2]\n",
    "n_0 = df1.loc[df1['smoker']==0,].shape[0]\n",
    "n_1 = df1.loc[df1['smoker']==1,].shape[0]\n",
    "n_0, n_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962a67a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-17T10:50:35.147913Z",
     "start_time": "2022-10-17T10:50:35.142925Z"
    }
   },
   "outputs": [],
   "source": [
    "df_under = pd.concat([df1.loc[df1['smoker']==0,].sample(n_1), df1.loc[df1['smoker']==1,]])\n",
    "df_under.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55706b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-17T10:50:44.097416Z",
     "start_time": "2022-10-17T10:50:44.086945Z"
    }
   },
   "outputs": [],
   "source": [
    "df_over = pd.concat([df1.loc[df1['smoker']==0,], df1.loc[df1['smoker']==1,].sample(n_0, replace=True)])\n",
    "df_over.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b408475",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-17T10:50:51.434262Z",
     "start_time": "2022-10-17T10:50:51.322671Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f07b6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-17T10:50:59.040568Z",
     "start_time": "2022-10-17T10:50:58.909064Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(df_under);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf2421d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-17T10:51:07.446224Z",
     "start_time": "2022-10-17T10:51:07.351551Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(df_over);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacaa67b",
   "metadata": {},
   "source": [
    "#### Undersampling and Oversampling using imbalanced-learn\n",
    "\n",
    "imbalanced-learn(imblearn) is a Python Package to tackle the curse of imbalanced datasets.\n",
    "\n",
    "It provides a variety of methods to undersample and oversample.\n",
    "\n",
    "a. Undersampling using Tomek Links:\n",
    "\n",
    "One of such methods it provides is called Tomek Links. Tomek links are pairs of examples of opposite classes in close vicinity.\n",
    "\n",
    "In this algorithm, we end up removing the majority element from the Tomek link which provides a better decision boundary for a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa9a490",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-17T10:45:26.020603Z",
     "start_time": "2022-10-17T10:45:26.018119Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip uninstall scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7fd115",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-17T10:45:26.612954Z",
     "start_time": "2022-10-17T10:45:26.566204Z"
    }
   },
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import TomekLinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c9c0d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-17T10:45:27.126166Z",
     "start_time": "2022-10-17T10:45:27.103744Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.sample(1000)\n",
    "X = df.iloc[:,:2]\n",
    "y = df.iloc[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0836797",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-17T10:45:29.795470Z",
     "start_time": "2022-10-17T10:45:29.784132Z"
    }
   },
   "outputs": [],
   "source": [
    "tl = TomekLinks(sampling_strategy='majority')\n",
    "X_tl, y_tl = tl.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6833c44",
   "metadata": {},
   "source": [
    "Oversampling using SMOTE:\n",
    "\n",
    "In SMOTE (Synthetic Minority Oversampling Technique) we synthesize elements for the minority class, in the vicinity of already existing elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dccb69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-17T10:51:26.755697Z",
     "start_time": "2022-10-17T10:51:26.662615Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(X)\n",
    "# plt.hist(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c3614b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-17T10:47:34.648355Z",
     "start_time": "2022-10-17T10:47:34.639926Z"
    }
   },
   "outputs": [],
   "source": [
    "n_0 = df.loc[df['smoker']==0,].shape[0]\n",
    "n_1 = df.loc[df['smoker']==1,].shape[0]\n",
    "n_0, n_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773f232d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-17T10:46:51.760847Z",
     "start_time": "2022-10-17T10:46:51.742185Z"
    }
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(sampling_strategy='minority')\n",
    "X_sm, y_sm = smote.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a760267e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-17T10:51:42.515443Z",
     "start_time": "2022-10-17T10:51:42.420170Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(X_tl)\n",
    "# plt.hist(y_tl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb279ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-17T10:51:50.533067Z",
     "start_time": "2022-10-17T10:51:50.431957Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(X_sm)\n",
    "# plt.hist(y_sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38b5361",
   "metadata": {},
   "source": [
    "There are a variety of other methods in the imblearn package for both undersampling(Cluster Centroids, NearMiss, etc.) and oversampling(ADASYN and bSMOTE) that you can check out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85528b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-17T10:28:29.044705Z",
     "start_time": "2022-10-17T10:28:29.041683Z"
    }
   },
   "source": [
    "### Multiple hypothesis testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dac2aa3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-17T10:28:29.883916Z",
     "start_time": "2022-10-17T10:28:29.881186Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd#Import the data samples\n",
    "from multipy.data import neuhaus#Import the FWER methods \n",
    "from multipy.fwer import bonferroni, holm_bonferroni#Import the FDR methods (LSU is the other name for BH method)\n",
    "from multipy.fdr import lsu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d604d324",
   "metadata": {},
   "source": [
    "Let’s assume we have 15 features, and we already did our hypothesis testing for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14da47e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-17T10:29:33.459525Z",
     "start_time": "2022-10-17T10:29:33.456503Z"
    }
   },
   "outputs": [],
   "source": [
    "pvals = neuhaus()\n",
    "df = pd.DataFrame({'Features': ['Feature {}'.format(i) for i in range(1,len(pvals)+1  )], 'P-value':pvals})\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e540bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-17T10:29:42.618096Z",
     "start_time": "2022-10-17T10:29:42.610889Z"
    }
   },
   "outputs": [],
   "source": [
    "# Now, let’s try the Bonferroni Correction to our data sample\n",
    "#Set the alpha level for your desired significant level\n",
    "df['bonferroni'] = bonferroni(pvals, alpha = 0.05)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dfe40f",
   "metadata": {},
   "source": [
    "With the function from MultiPy, we end up either with True or False results. True means we Reject the Null Hypothesis, while False, we Fail to Reject the Null Hypothesis.\n",
    "\n",
    "From the Bonferroni Correction method, only three features are considered significant. Let’s try the Holm-Bonferroni method to see if there is any difference in the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2fc8e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-17T10:31:50.837235Z",
     "start_time": "2022-10-17T10:31:50.828272Z"
    }
   },
   "outputs": [],
   "source": [
    "df['holm_bonferroni'] = holm_bonferroni(pvals, alpha = 0.05)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ccd1be",
   "metadata": {},
   "source": [
    "No change at all in the result. It seems the conservative method FWER has restricted the significant result we could get. Let’s see if there is any difference if we use the BH method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde65028",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-17T10:30:54.035395Z",
     "start_time": "2022-10-17T10:30:54.007252Z"
    }
   },
   "outputs": [],
   "source": [
    "#set the q parameter to the FDR rate you want\n",
    "df['benjamin_hochberg'] = lsu(pvals, q=0.05) # q = desired FDR\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65427db2",
   "metadata": {},
   "source": [
    "The less strict method FDR resulted in a different result compared to the FWER method. In this case, we have four significant features. The FDR is proven to laxer to find the features, after all.\n",
    "\n",
    "If you want to learn more about the methods available for Multiple Hypothesis Correction, you might want to visit the MultiPy homepage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba9c618",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-17T10:31:18.851588Z",
     "start_time": "2022-10-17T10:31:18.843677Z"
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.stats.multitest import multipletests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb446aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-17T10:31:19.850176Z",
     "start_time": "2022-10-17T10:31:19.847007Z"
    }
   },
   "outputs": [],
   "source": [
    "reject, p_value_corrected, sidak_corr, bonf_corr = multipletests(pvals, alpha=0.05, method='fdr_bh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3bfe2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-17T10:31:25.583859Z",
     "start_time": "2022-10-17T10:31:25.571659Z"
    }
   },
   "outputs": [],
   "source": [
    "reject, p_value_corrected, sidak, bonferroni =  multipletests(pvals, alpha = 0.05, method='sidak')\n",
    "df['sidak'] = reject\n",
    "reject, p_value_corrected, sidak, bonferroni =  multipletests(pvals, alpha = 0.05, method='holm-sidak')\n",
    "df['holm-sidak'] = reject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb734ea2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-17T10:31:34.910249Z",
     "start_time": "2022-10-17T10:31:34.901887Z"
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853a89d1",
   "metadata": {},
   "source": [
    "\n",
    "    reject — булевский массив длины 𝑚, в котором True — нулевую гипотезу можно отвергнуть и False — если нельзя\n",
    "    pvals_corrected — массив длины 𝑚 со скорректированными p-value\n",
    "    alphacSidak — поправка Шидака\n",
    "    alphacBonf — поправка Бонферонни\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0695d95",
   "metadata": {},
   "source": [
    "\n",
    "Поправка Шидака\n",
    "\n",
    "\n",
    "Как и в поправке Бонферонни, поправка Шидака корректирует $\\alpha$ (уровни значимости для проверки единичных гипотез). Она также сохраняет $F W E R \\leq \\alpha$\n",
    "Посчитаем, чему равна поправка Шидака. $P(V \\leq 1)=1-P(V=0) \\leq 1-\\left(1-\\alpha_{1}\\right)^{m}=\\alpha$, где $\\alpha-$ заданный нами уровень значимости для семейства гипотез и $\\alpha_{1}-$ искомый уровень значимости для проверки каждой единичной гипотезы.\n",
    "Выразим $\\alpha_{1}$ через $\\alpha$ и получим $\\alpha_{1}=1-(1-\\alpha)^{1 / m} \\mid$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5814b4b",
   "metadata": {},
   "source": [
    "Метод Шидака-Холма\n",
    "\n",
    "\n",
    "Как и в предыдущем методе, где отметился Холм, используется итерационная корректировка р-value. Аналогично сортируем наши р-value по возрастанию и корректируем их согласно поправке Шидака: $\\alpha_{1}=1-(1-\\alpha)^{\\frac{\\pi}{m}}$\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "\\alpha_{i}=1-(1-\\alpha)^{\\frac{\\alpha}{m-l+1}} \\\\\n",
    "\\ldots \\\\\n",
    "\\alpha_{m}=\\alpha\n",
    "\\end{array}\n",
    "$$\n",
    "Обладает несколькими свойствами:\n",
    "1. Контролирует FWER на уровне значимости $\\alpha$, если статистики независимы в совокупности.\n",
    "2. Если статистики независимы в совокупности, нельзя построить контролирующую FWER на уровне $\\alpha$ процедуру мощнее, чем метод Шидака-Холма.\n",
    "3. При больших $m$ мало отличается от метода Холма"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
