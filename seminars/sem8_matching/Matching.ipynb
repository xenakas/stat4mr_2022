{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T14:17:05.174757Z",
     "start_time": "2022-11-14T14:17:05.171834Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyAgrum as gum\n",
    "import pyAgrum.lib.notebook as gnb\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T14:17:05.189968Z",
     "start_time": "2022-11-14T14:17:05.187949Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install pyAgrum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are given this database coming from the historical records of a hospital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T14:17:05.388984Z",
     "start_time": "2022-11-14T14:17:05.270525Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"sample\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How much will number of deaths change? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much will the number of deaths change if we decide to treat everybody from now on versus not treating anyone?\n",
    "\n",
    "Our first instinct is to simply compute the percentages of deaths of the treated population versus the untreated population using the data we are given, and simply subtract one from the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T14:17:07.071627Z",
     "start_time": "2022-11-14T14:17:07.039966Z"
    }
   },
   "outputs": [],
   "source": [
    "treated = df[df[\"treatment\"]==1]\n",
    "untreated = df[df[\"treatment\"]==0]\n",
    "\n",
    "percentage_dead_treated = treated[\"dead\"].mean()\n",
    "percentage_dead_treated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T14:17:15.940194Z",
     "start_time": "2022-11-14T14:17:15.935165Z"
    }
   },
   "outputs": [],
   "source": [
    "percentage_dead_untreated= untreated[\"dead\"].mean()\n",
    "percentage_dead_untreated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T14:17:19.284906Z",
     "start_time": "2022-11-14T14:17:19.281438Z"
    }
   },
   "outputs": [],
   "source": [
    "percentage_dead_treated - percentage_dead_untreated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Response: The percentage of deads will increase a 8.5% if we decide to treat everybody (it will jump from 23.56% to 32.08%). Consequently, we SHOULD NOT TREAT our population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result would then counsel us that we should not treat our population, but our common sense may be “tingling” and making us wonder if that is really the case. And rightly so, because we overlooked a tiny but very crucial detail, and that is that the treatment is not distributed equally between the smoking population and the non-smoking one. In other words, treatment is not randomized, and hence, this way of calculating the ATE is simply wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BUT: The real effect of this treatment then, is a lowered percentage of deaths by an 8.3%, exactly the contrary that we got in the first calculations. Therefore, the real answer is that we should absolutely treat all patients. Now, to have this exact percentages is very rare. We can certainly approximate ATE with a good observational dataset, applying the same system, but the problem with this method arises when the dataset has a high dimensional set of covariates. Imagine a set of covariates in the hundreds. Computing this formula for all the covariates would be too cumbersome. It is not the case of this very simple example, but in the real world it is a very common circumstance. What should we do in those cases then?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can use matching on propensity score\n",
    "\n",
    "Well, that is something that Rosenbaum and Rubin tried to solve in 1983 proposing that we use the propensity score (or probability of getting a treatment given a set of covariates) as a balancing score. Their reasoning goes as follows. A balancing score is any function of the set of covariates that captures all the information of the set that is dependent on treatment. Such a balancing score would allow us to model the relation between the confounders and treatment in a relatively simple way.|And the minimal expression of a balancing score is the propensity score.\n",
    "\n",
    "Computing the propensity score is relatively simple, even in high dimensional sets of covariates. In these casps what we could do is model it using logistic regression with treatment as the target variable. But to be able to use this propensity score in the methods we will review next, there are some constraints.\n",
    "\n",
    "To ensure this, formally, there are 2 assumptions that need to be met:\n",
    "- The stable unit-treatment value assumption (SUTVA): Any outcome of any unit of the sample is independent of the treatment assignment to other units.\n",
    "- Treatment assignment should be strongly ignorable given a set of covariates: It is if every unit of the sample has a chance (even if small) of receiving each treatment, and if the treatment assignment and outcome are conditionally independent given that set of covariates.\n",
    "\n",
    "If these two assumptions are met, we are good to go with the methods we are going to review now.\n",
    "\n",
    "### We start computing the propensities of each sample. \n",
    "\n",
    "\n",
    "As before, we will review the methods applying them to our specific example. As stated earlier, we were able to compute the exact ATE because we knew the accurate probabilities of every variable combination. These methods assume that we don't know them, because with high dimensional sets of covariates that would be nearly impossible. Therefore, we will compare their estimations of the value of ATE to the known true result.\n",
    "\n",
    "\n",
    "Let's start by computing our propensity score values. It is defined formally as follows:\n",
    "\n",
    "\\begin{align}\n",
    "e(x)=p(z=1|x)\n",
    "\\end{align}\n",
    "\n",
    "**In our case:**\n",
    "\n",
    "Where $x$ is a specific combination of the set of covariates and $z=1$ equates to receiving treatment.\n",
    "And in our specific case it is translated to:\n",
    "\n",
    "\\begin{align}\n",
    "e(smoker)=p(treatment=1|smoker)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### We start by computing the propensity of each patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T14:18:37.879912Z",
     "start_time": "2022-11-14T14:18:37.859228Z"
    }
   },
   "outputs": [],
   "source": [
    "n_smokers = len(df[df[\"smoker\"]==1])\n",
    "n_treated_smokers = len(df[(df[\"smoker\"]==1) & (df[\"treatment\"]==1)])\n",
    "e_smoker = n_treated_smokers/n_smokers\n",
    "print(e_smoker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T14:18:43.743692Z",
     "start_time": "2022-11-14T14:18:43.719379Z"
    }
   },
   "outputs": [],
   "source": [
    "n_non_smokers = len(df[df[\"smoker\"]==0])\n",
    "n_treated_non_smokers = len(df[(df[\"smoker\"]==0) & (df[\"treatment\"]==1)])\n",
    "e_non_smoker = n_treated_non_smokers/n_non_smokers\n",
    "print(e_non_smoker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T14:18:51.747641Z",
     "start_time": "2022-11-14T14:18:51.730174Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"propensity\"] = df[\"smoker\"]*e_smoker + (1-df[\"smoker\"])*e_non_smoker\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case there are only 2 possible values, since our confounder is binary. Once computed and added to our dataframe we can pair match in two different ways.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We match each treated patient with a control patient that has the same propensity score. However, we reduce the number of treated patients for efficiency reasons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this version of pair matching, we couple each treated patient with a control patient that has the same propensity score. For this example, we reduce the number of treated patients for efficiency reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T14:19:10.069060Z",
     "start_time": "2022-11-14T14:19:10.045902Z"
    }
   },
   "outputs": [],
   "source": [
    "treated = df[df[\"treatment\"]==1].sample(100)\n",
    "treated = treated.reset_index(drop=True)\n",
    "treated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We match each treated patient with a randomly sampled untreated patient with the same propensity score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, every treated patient gets a randomly sampled untreated patient with the same propensity score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T14:19:22.100946Z",
     "start_time": "2022-11-14T14:19:20.819161Z"
    }
   },
   "outputs": [],
   "source": [
    "untreated = df[df[\"treatment\"]==0]\n",
    "matched_control = []\n",
    "def add_matched_control(patient):\n",
    "    control_patient = untreated[untreated[\"propensity\"]==patient[\"propensity\"]].sample().iloc[0]\n",
    "    matched_control.append(control_patient)\n",
    "    \n",
    "treated.apply(add_matched_control, axis=1)\n",
    "matched_control_df = pd.DataFrame(matched_control).reset_index(drop=True)\n",
    "matched_control_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, every treated patient gets a randomly sampled untreated patient with the same propensity score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T14:19:40.518000Z",
     "start_time": "2022-11-14T14:19:40.506753Z"
    }
   },
   "outputs": [],
   "source": [
    "paired_sample = treated.join(matched_control_df, rsuffix=\"_control\")\n",
    "paired_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T14:19:44.649356Z",
     "start_time": "2022-11-14T14:19:44.565288Z"
    }
   },
   "outputs": [],
   "source": [
    "ATE = (paired_sample[\"dead\"]-paired_sample[\"dead_control\"]).mean()\n",
    "ATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get as a result that the percentage of deaths is effectively estimated to decrease, which is good as it is in line with the real effect computed before. However, we know that the real ATE is 0.083. This result is clearly biased.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Response: The percentage of deads is estimated to decrease!!!**\n",
    "\n",
    "Consequently, we should treat our population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**However, we know that the real ATE is 0.083. Why is this calculation so inacurate? ...**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's take an alternative path to use propensity score matching\n",
    "We start by taking a look at the distribution of propensity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T14:20:12.610329Z",
     "start_time": "2022-11-14T14:20:12.402140Z"
    }
   },
   "outputs": [],
   "source": [
    "df.hist(\"propensity\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, we have a majority of the patients getting a low propensity score.\n",
    "\n",
    "We can split patients into two groups, those with high propensity (> 0.5) and those with low propensity (<=0.5):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can split patients into two groups, those with high propensity (> 0.5) and those with low propensity (<=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T14:20:16.526329Z",
     "start_time": "2022-11-14T14:20:16.493614Z"
    }
   },
   "outputs": [],
   "source": [
    "high_propensity = df[df[\"propensity\"]>0.5]\n",
    "low_propensity = df[df[\"propensity\"]<=0.5]\n",
    "counts = np.array([len(low_propensity), len(high_propensity)])\n",
    "percentages= counts / np.sum(counts)\n",
    "percentages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a paired sample "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we build a paired sample, but unlike before both treated and untreated populations are sampled from high or low propensity score population at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T14:20:42.900175Z",
     "start_time": "2022-11-14T14:20:21.418642Z"
    }
   },
   "outputs": [],
   "source": [
    "n_samples=1000\n",
    "samples = []\n",
    "for i in range(n_samples):\n",
    "    is_high_propensity = random.random()>percentages[0]\n",
    "    if is_high_propensity:\n",
    "        treated_patient = high_propensity[high_propensity[\"treatment\"]==1].sample().iloc[0]\n",
    "        untreated_patient = high_propensity[high_propensity[\"treatment\"]==0].sample().iloc[0]\n",
    "    else:\n",
    "        treated_patient = low_propensity[low_propensity[\"treatment\"]==1].sample().iloc[0]\n",
    "        untreated_patient = low_propensity[low_propensity[\"treatment\"]==0].sample().iloc[0]\n",
    "    samples.append((treated_patient, untreated_patient))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T14:20:44.439988Z",
     "start_time": "2022-11-14T14:20:44.431616Z"
    }
   },
   "outputs": [],
   "source": [
    "individual_treatment_effect = np.zeros(n_samples)\n",
    "i = 0\n",
    "for t, c in samples:\n",
    "    individual_treatment_effect[i] = t[\"dead\"]-c[\"dead\"]\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T14:20:46.023908Z",
     "start_time": "2022-11-14T14:20:46.020105Z"
    }
   },
   "outputs": [],
   "source": [
    "ATE=individual_treatment_effect.mean()\n",
    "ATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After calculating the value of ATE for this new paired dataset, we can see that the value is much less biased than with the first version of the pair matching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have managed to reduce the bias by sampling first from the distribution of propensity scores. \n",
    "\n",
    "Can we do better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subclassification on propensity score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the approach is related to the second version of pair matching, as it relies on the distribution of the propensity score. The hist of it is that we can subdivide our population by the categories of a factorized by ranges propensity score. With our population classified in each category (in this case only two, as the covariate is binary), we only need to compute the ATE for each subclass, and then apply the next formula:\n",
    "$$\n",
    "A T E=\\text { Percentage }_{1} * \\text { ATE }_{1}+\\ldots+\\text { Percentage }_{n} * A T E_{n}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T10:06:00.830882Z",
     "start_time": "2022-11-14T10:06:00.812999Z"
    }
   },
   "outputs": [],
   "source": [
    "n_smokers = len(df[df[\"smoker\"]==1])\n",
    "n_treated_smokers = len(df[(df[\"smoker\"]==1) & (df[\"treatment\"]==1)])\n",
    "e_smoker = n_treated_smokers/n_smokers\n",
    "print(e_smoker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T10:06:01.957280Z",
     "start_time": "2022-11-14T10:06:01.932416Z"
    }
   },
   "outputs": [],
   "source": [
    "n_non_smokers = len(df[df[\"smoker\"]==0])\n",
    "n_treated_non_smokers = len(df[(df[\"smoker\"]==0) & (df[\"treatment\"]==1)])\n",
    "e_non_smoker = n_treated_non_smokers/n_non_smokers\n",
    "print(e_non_smoker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the propensity score for each patient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T10:06:03.110729Z",
     "start_time": "2022-11-14T10:06:02.984190Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"propensity\"]=df[\"smoker\"]*e_smoker + (1-df[\"smoker\"])*e_non_smoker\n",
    "df.hist(\"propensity\")\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T10:06:04.334828Z",
     "start_time": "2022-11-14T10:06:04.308561Z"
    }
   },
   "outputs": [],
   "source": [
    "high_propensity = df[df[\"propensity\"]==e_smoker]\n",
    "low_propensity = df[df[\"propensity\"]==e_non_smoker]\n",
    "counts = np.array([len(low_propensity),len(high_propensity)])\n",
    "percentages= counts / np.sum(counts)\n",
    "percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T10:06:05.452941Z",
     "start_time": "2022-11-14T10:06:05.439973Z"
    }
   },
   "outputs": [],
   "source": [
    "##################################################\n",
    "\n",
    "high_propensity_treated = high_propensity[high_propensity[\"treatment\"]==1]\n",
    "high_propensity_untreated = high_propensity[high_propensity[\"treatment\"]==0]\n",
    "ATE_high_propensity = high_propensity_treated[\"dead\"].mean()-high_propensity_untreated[\"dead\"].mean()\n",
    "ATE_high_propensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T10:06:06.510480Z",
     "start_time": "2022-11-14T10:06:06.489843Z"
    }
   },
   "outputs": [],
   "source": [
    "low_propensity_treated = low_propensity[low_propensity[\"treatment\"]==1]\n",
    "low_propensity_untreated = low_propensity[low_propensity[\"treatment\"]==0]\n",
    "ATE_low_propensity = low_propensity_treated[\"dead\"].mean()-low_propensity_untreated[\"dead\"].mean()\n",
    "ATE_low_propensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T10:06:07.588078Z",
     "start_time": "2022-11-14T10:06:07.585701Z"
    }
   },
   "outputs": [],
   "source": [
    "ATE = percentages[0] * ATE_low_propensity + percentages[1] * ATE_high_propensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T10:06:08.654604Z",
     "start_time": "2022-11-14T10:06:08.651040Z"
    }
   },
   "outputs": [],
   "source": [
    "ATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After doing so our results is very very close to the known true value of ATE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Response: The percentage of deads is estimated to decrease!!!**\n",
    "\n",
    "Consequently, we should treat our population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**USEFUL LINKS**\n",
    "\n",
    "https://towardsdatascience.com/a-hands-on-introduction-to-propensity-score-use-for-beginners-856302b632ac\n",
    "\n",
    "https://github.com/cerquide/propensity_score_talk\n",
    "\n",
    "======\n",
    "\n",
    "https://analyticsmayhem.com/digital-analytics/propensity-score-matching-python/\n",
    "\n",
    "https://github.com/konosp/propensity-score-matching/blob/main/propensity_score_matching_v2.ipynb\n",
    "\n",
    "https://matheusfacure.github.io/python-causality-handbook/11-Propensity-Score.html"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
